{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":85,"metadata":{"id":"-B1UMwVDdjQF","executionInfo":{"status":"ok","timestamp":1684274582777,"user_tz":-180,"elapsed":6,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}}},"outputs":[],"source":["import numpy as np\n","import cv2 as cv\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","#from patchify import patchify, unpatchify\n","%matplotlib inline\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, models\n","from PIL import Image"]},{"cell_type":"code","source":["# Functions needed\n","def gram_matrix(tensor):\n","    _, d, h, w = tensor.size()\n","    tensor = tensor.view(d, h * w)\n","    gram = torch.mm(tensor, tensor.t())\n","    return gram\n","\n","def get_features(image, model, layers):\n","    features = {}\n","    for name, layer in model._modules.items():\n","        image = layer(image)\n","        if int(name) in layers:\n","            features[int(name)] = image\n","    return features\n","\n","def im_convert(tensor):\n","    image = tensor.to(\"cpu\").clone().detach()\n","    image = image.numpy().squeeze()\n","    image = image.transpose(1,2,0)\n","    image = image * np.array((0.229, 0.224, 0.225))\n","    image = image + np.array((0.485, 0.456, 0.406))\n","    image = image.clip(0, 1)\n","\n","    return image\n","\n","def save_image(tensor, path):\n","    image = im_convert(tensor)\n","    image = Image.fromarray((image * 255).astype(np.uint8))\n","    image.save(path)"],"metadata":{"id":"EKuNWifviYLw","executionInfo":{"status":"ok","timestamp":1684274582778,"user_tz":-180,"elapsed":7,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["# Load VGG19, instantiate constants\n","vgg = models.vgg19(pretrained=True).features\n","# Define content and style layers\n","content_layers = [19] # This corresponds to 'conv_4' in VGG19\n","style_layers = [0, 5, 10, 19, 28] # These correspond to 'conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'\n","# Freeze parameters. We don't need to train any layers\n","for param in vgg.parameters():\n","    param.requires_grad_(False)\n","# Define transformations\n","transform = transforms.Compose([transforms.Resize(256), \n","                                transforms.ToTensor()])\n","# Set content and style weighting\n","alpha = 1\n","beta = 1e6\n","# Define weights for style layers\n","style_weights = {0: 1.,\n","                 5: 0.75,\n","                 10: 0.2,\n","                 19: 0.2,\n","                 28: 0.2}\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AquD8wvYjin2","executionInfo":{"status":"ok","timestamp":1684274584843,"user_tz":-180,"elapsed":2071,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}},"outputId":"07a3e1b3-4e58-4e12-e93a-7ece6d9b6631"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["#NORMALIZE IMAGE\n","\n","# loader = transforms.Compose([\n","#     transforms.Resize((600, 600)), \n","#     transforms.ToTensor(), \n","# ])\n","\n","style_path='/content/drive/MyDrive/Project_image_folder/style.png'\n","\n","style = transform(Image.open(style_path)).unsqueeze(0)\n","#style=cv.resize(cv.imread(style_path), (600,600))\n","#cv2_imshow(style)\n","content_path='/content/drive/MyDrive/Project_image_folder/content.png'\n","content = transform(Image.open(content_path)).unsqueeze(0)\n","#content=cv.resize(cv.imread(content_path), (600,600))\n","#cv2_imshow(content)\n"],"metadata":{"id":"wpAz2sdxfjYj","executionInfo":{"status":"ok","timestamp":1684274584844,"user_tz":-180,"elapsed":5,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["#Apply VGG19\n","#Need PIL Images\n","def apply_network(content, style, optimization_iteration):\n","  # Compute content and style features\n","  content_features = get_features(content, vgg, content_layers)\n","  style_features = get_features(style, vgg, style_layers)\n","\n","  # Compute gram matrix for each layer in style_features\n","  style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n","  # Initialize target image with content image\n","  target = content.clone().requires_grad_(True)\n","  # Define optimizer\n","  optimizer = optim.Adam([target], lr=0.003)\n","  # Iteration for optimization\n","  for i in range(1, optimization_iteration):\n","    target_features = get_features(target, vgg, style_layers)\n","    content_loss = 0\n","    content_loss = torch.mean((target_features[content_layers[0]] - content_features[19])**2)\n","    style_loss = 0\n","    for layer in style_weights:\n","        target_feature = target_features[layer]\n","        target_gram = gram_matrix(target_feature)\n","        _, d, h, w = target_feature.shape\n","        style_gram = style_grams[layer]\n","        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n","        style_loss += layer_style_loss / (d * h * w)\n","    total_loss = alpha * content_loss + beta * style_loss\n","    optimizer.zero_grad()\n","    total_loss.backward()\n","    optimizer.step()\n","  return target\n","\n","target=apply_network(content, style, 1)\n"],"metadata":{"id":"fox8auMdltiH","executionInfo":{"status":"ok","timestamp":1684274592272,"user_tz":-180,"elapsed":7432,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["#Save\n","prework_path=\"/content/drive/MyDrive/Project_image_folder/prework.jpg\"\n","save_image(target, prework_path)\n","prework=cv.imread(prework_path)\n","prework=cv.resize(prework, (600,600))\n","cv2_imshow(prework)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"BdRjbRL9y_Ib","executionInfo":{"status":"error","timestamp":1684275445738,"user_tz":-180,"elapsed":595,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}},"outputId":"234eefcf-5658-4393-939e-91c6761278e5"},"execution_count":96,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-96-94cdbabbb0a5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprework_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Project_image_folder/prework.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprework_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprework_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'show'"]}]},{"cell_type":"code","source":["# split in patches\n","\n","patch_size=3\n","\n","def split_into_patches(M, N, split_image):\n","  return [split_image[x:x+M,y:y+N] for x in range(0,split_image.shape[0],M) for y in range(0,split_image.shape[1],N)];\n","\n","style=cv.resize(cv.imread(style_path), (600,600))\n","content=prework\n","patches_style=split_into_patches(patch_size, patch_size, style)\n","patches_img=split_into_patches(patch_size, patch_size, content)"],"metadata":{"id":"in2TOLRDhDgb","executionInfo":{"status":"ok","timestamp":1684274592274,"user_tz":-180,"elapsed":15,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["#Main?\n","def argmax(result_lst):\n","  print(result_lst)\n","  out=0\n","  max=result_lst[0]\n","  for i in range(len(result_lst)):\n","    if result_lst[i]>max:\n","      max=result_lst[i]\n","      out=i\n","  return out\n","\n","def unite(img_patch, stl_patch):\n","  img_patch_pil = transform(Image.fromarray(cv.cvtColor(img_patch, cv.COLOR_BGR2RGB))).unsqueeze(0)\n","  stl_patch_pil = transform(Image.fromarray(cv.cvtColor(stl_patch, cv.COLOR_BGR2RGB))).unsqueeze(0)\n","  save_image(img_patch_pil, \"/content/drive/MyDrive/Project_image_folder/some.jpg\")\n","  pil_result=apply_network(img_patch_pil, stl_patch_pil, 1).detach().numpy()\n","\n","  return cv.cvtColor(np.array(pil_result), cv.COLOR_RGB2BGR)"],"metadata":{"id":"u-XueMDqciUQ","executionInfo":{"status":"ok","timestamp":1684276134065,"user_tz":-180,"elapsed":3,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["#Formula\n","output=[]\n","for img in patches_img:\n","  cofficients=[]\n","  for stl in patches_style:\n","    cofficients.append(np.argmax(np.dot(img, stl)/np.dot(img, img)/np.dot(stl, stl)))\n","  output.append(unite(img, patches_style[argmax(cofficients)]))\n","output=np.array(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1CaQdctasEHGLuHwp3SsxAY7pAkFrgeMu"},"id":"ba7SaHw7ck9O","outputId":"abc210a2-2dca-4e02-fdfb-8693468b1a89"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["img  = Image.fromarray(output[0])\n","# Saving the image\n","img.save(\"/content/drive/MyDrive/Project_image_folder/work.jpg\")"],"metadata":{"id":"Sm8fgGl2iIi-","executionInfo":{"status":"aborted","timestamp":1684274592780,"user_tz":-180,"elapsed":8,"user":{"displayName":"Арсеній Казимир","userId":"02352925006576352530"}}},"execution_count":null,"outputs":[]}]}